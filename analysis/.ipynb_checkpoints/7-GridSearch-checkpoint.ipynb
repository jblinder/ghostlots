{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinblinder/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil.parser\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from math import sqrt\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import pipeline, preprocessing, neighbors, model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "df_pluto = pd.read_pickle('../data/pkl/df_pluto-cat-feat-2013-2017.pkl')\n",
    "\n",
    "# Create holdout set of current vacant lots that are in specific zip code\n",
    "#df_pluto_holdout = df_pluto[(df_pluto['LandUse']==11)&(df_pluto['PolicePrct_75.0'] == 1)]\n",
    "\n",
    "# Remove columns from train/test set\n",
    "df_pluto = df_pluto[df_pluto.columns.difference(['BldgClass','FireComp','IrrLotCode','Sanborn','SplitZone','ZoneDist1'])]\n",
    "df_pluto = df_pluto[df_pluto.columns.difference(['APPBBL','AreaSource','BBL','BoroCode','CondoNo','TaxMap'])]\n",
    "df_pluto = df_pluto[df_pluto.columns.difference(['new_bldg_prob','ZoneCodeChanged','LandUse','LandUse2016','ZoneCodeBecameCondo'])]\n",
    "df_pluto = df_pluto[df_pluto.columns.difference(['Cat_LandUse2016_11.0','Cat_LandUse2016_10.0','Cat_LandUse2016_9.0','Cat_LandUse2016_8.0','Cat_LandUse2016_7.0','Cat_LandUse2016_6.0','Cat_LandUse2016_5.0','Cat_LandUse2016_4.0','Cat_LandUse2016_3.0','Cat_LandUse2016_2.0','Cat_LandUse2016_1.0'])]\n",
    "df_pluto = df_pluto.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "# Subset data\n",
    "df_pluto_subset = df_pluto[::50]\n",
    "\n",
    "# Get coefs\n",
    "y = df_pluto_subset['ZoneCodeWasVacant'].astype(int)\n",
    "X = df_pluto_subset[df_pluto_subset.columns.difference(['ZoneCodeWasVacant'])]\n",
    "\n",
    "# Create test train split\n",
    "xtrain, xtest, ytrain, ytest= train_test_split(X,y)\n",
    "\n",
    "# Smote train/ test data\n",
    "sm = SMOTE(random_state=42)\n",
    "xtrain, ytrain = sm.fit_sample(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale train/test/holdout values values\n",
    "scaler = StandardScaler().fit(xtrain)\n",
    "xtrain_s = scaler.transform(xtrain)\n",
    "xtest_s = pd.DataFrame(scaler.transform(xtest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\"../data/pkl/rf-alldata-final-scaled.pkl\", \"rb\") as input_file:\n",
    "    rf = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#build parameter grid\n",
    "n_estimators_range = list(range(10,100,10))\n",
    "max_features_options = ['sqrt', 'log2', None]\n",
    "min_samples_split_range = list(np.linspace(0.01,.1,5))\n",
    "min_impurity_decrease_range = list(np.linspace(0.01,.1,5))\n",
    "tuned_parameters = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\n",
    "\n",
    "param_grid = dict(n_estimators = n_estimators_range,\n",
    "                  max_features = max_features_options,\n",
    "                  min_samples_split = min_samples_split_range, \n",
    "                  min_impurity_decrease = min_impurity_decrease_range)\n",
    "\n",
    "#fit grid\n",
    "grid = GridSearchCV(RandomForestClassifier(n_jobs=-1), param_grid, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(xtrain_s,ytrain)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(-2, 4, 1)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0,n_jobs=-1)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(xtrain_s, ytrain)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "# Predict target vector\n",
    "best_model.predict(xtest_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "solver_options = ['newton-cg', 'lbfgs', 'liblinear', 'sag']\n",
    "multi_class_options = ['ovr', 'multinomial']\n",
    "class_weight_options = ['None', 'balanced']\n",
    "c = [0.001, 0.01, 0.1, 1, 10, 100, 1000] \n",
    "param_grid = dict(solver = solver_options, multi_class = multi_class_options, C = c)\n",
    "                  \n",
    "clf = GridSearchCV(logreg(penalty='l2'), param_grid)grid = \n",
    "                  GridSearchCV(LogisticRegression, param_grid, cv=12, scoring ='accuracy')\n",
    "\n",
    "                  \n",
    "\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "GridSearchCV(cv=None,\n",
    "             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n",
    "               dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n",
    "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
